---
title: "Data Engineering Manual"
author: "Ettore Miglioranza"
format:
  html:
    toc: true                # Table of contents on the side
    toc-depth: 2             # Shows headings up to level 2 (##)
    number-sections: true    # Automatically numbers sections
    theme: flatly            # Bootstrap theme (others: cosmo, darkly, etc.)
    code-fold: true          # Collapsible code blocks
    highlight-style: github  # Code syntax highlighting style
    css: styles.css          # (optional) your custom CSS file
editor: visual
---

# Object Storage

## What is Object Storage?

Object Storage is a way of storing and managing large amounts of unstructured data. Each **object** contains:

-   The data (actual content)
-   Metadata
-   A unique identifier
-   Attributes about the object itself

In the world of object storage, all data is stored as objects inside containers called buckets. Notably, these buckets are virtually limitless in size, capable of scaling to hold billions of objects without performance degradation.

Another key feature is that buckets can be used for cold storage—that is, to store data that may rarely be accessed but must never be deleted. Users interact with buckets indirectly through APIs, rather than accessing them directly. When a user uploads an object via an API, the system accepts the object and places it into a designated bucket.

While the bucket is a logical (virtual) construct, the object storage system handles the physical storage behind the scenes. For example, it might replicate the object across three separate physical devices. This replication strategy ensures both data integrity and data durability, protecting against hardware failure and data loss.

The **logical bucket**: Exists in the control plane, meaning it’s about how data is managed, not where it physically lives.

**Physical Buckets** (or **Storage Nodes**): is the actual physical storage where the data is written—like hard drives, SSDs, or other hardware across data centers. Invisible to the user, objects are often replicated across multiple physical locations for durability and availability, and managed by the storage system, which handles where and how data is stored behind the scenes.

## Evaluation of object cloud storage

A key concept in object storage is response time. The higher the performance you require—meaning the faster the response time—the higher the cost. At the lowest cost tier, known as cold storage, data is stored with the assumption that it will rarely, if ever, be accessed. On the other end of the spectrum, storing frequently accessed data—such as images, videos, or other assets served by a website—falls into coldish storage (*IBM technology definition*), which is more expensive due to the higher performance requirements.

In both cases, users interact with the system through APIs, and don’t need to manage the backend infrastructure. For front-end developers, this abstraction allows them to focus on building interfaces while relying on the object storage platform to handle scalability, durability, and distribution.

## Real-world Use Cases

-   **Video Streaming**: When hosting videos for global streaming, performance and low latency are critical. Instead of storing the object in just three devices, the system replicates the video across multiple geographic locations—for example, three different data centers in seven countries. This global replication reduces latency and improves the user experience.

-   **File Sharing and Collaboration**: Consider a company with a global workforce collaborating on spreadsheets or presentations. Object storage supports versioning, allowing multiple users to work on the same file.

User1 updates a document in London -\> User2 downloads it, makes changes, and reuploads a new version -\> User3 in Singapore does the same.

Each modification updates the object's metadata, which tracks versions and changes, enabling seamless collaboration without emailing files back and forth. This approach is more efficient, more secure, and easier to control through access permissions.

-   **Archival Projects**: For long-term archival, such as storing historical photographs for public access, object storage is ideal. The "write once, read many" pattern fits perfectly, as the data is rarely modified but must remain available and secure over time.

Common Object Storage services include:

-   **Amazon S3**
-   **Google Cloud Storage**
-   **Azure Blob Storage**

## Key Advantages summaries

-   Virtually unlimited scalability\
-   Cost-efficient\
-   Accessible via REST APIs\
-   Ideal for data lakes, backups, and archiving

## So then, when to Use It?

Object Storage is ideal for:

-   Storing raw data
-   Saving logs, images, and videos
-   Integrating with Big Data tools (e.g., Spark, Presto)

# Object Storage: Real-World vs MinIO Simulation

In this project, we use **MinIO** as our object storage solution because it is lightweight, high-performance, and fully compatible with the Amazon S3 API. It allows us to simulate a production-grade object storage environment **locally or on-premises** with full control over replication, versioning, and access policies.

We run MinIO inside a **Docker container** to ensure an isolated, portable, and easy-to-deploy setup. Docker simplifies infrastructure management and allows us to spin up or reset environments quickly during development.

To interact with MinIO from our Python application, we use **Boto3**, the official AWS SDK for Python. Even though it's designed for Amazon S3, Boto3 works seamlessly with MinIO thanks to its S3-compatible API. This allows us to perform actions like uploading, downloading, or listing files programmatically without dealing with low-level storage operations.

## Real-World Object Storage on AWS

In the real-world, cloud providers like AWS S3 use **object storage** to store data as objects (files) within **buckets**. Each object is composed of data and metadata, and is assigned a unique **key** to access it. These objects are often **split into chunks** for efficient storage and retrieval. Cloud systems, such as AWS S3, store these chunks across **multiple physical devices** or **data centers** in **distributed systems** to ensure **high availability** and **fault tolerance**. The objects are typically replicated to multiple **data centers** or **availability zones** to provide **redundancy** in case of hardware failure or network issues. This replication ensures that if one data center fails, the object is still accessible from another replica in a different data center, providing seamless availability and durability. The object storage system can scale horizontally, meaning the data can be spread across multiple devices to increase storage capacity and system resilience.

## MinIO as a Simulation of Real-World Object Storage

MinIO is an **open-source, high-performance object storage system** that is **fully compatible** with the AWS S3 API. While AWS S3 operates in a **distributed, multi-node cloud environment**, MinIO allows you to simulate a similar environment on a **single physical node** (your local machine). In MinIO, objects are stored within **buckets**, and large objects can be **split into chunks** for efficient storage. However, unlike AWS S3, MinIO on a local machine operates as a **single-node system**, meaning all data (objects and chunks) are stored on your local storage device, without distribution across multiple devices or data centers. MinIO is a good **simulation of the real-world S3** because it mimics the S3 API, meaning tools like `boto3` (used to interact with AWS S3) work seamlessly with MinIO, and the overall logic of uploading, storing, and retrieving objects is the same.

## Docker Setup for MinIO Simulation

To simulate a real-world object storage system on our local machine using MinIO, we set up a **Docker container** to run MinIO as a **single-node object storage service**. The MinIO container is like a logical node in our object storage system, where it interacts with the physical storage devices (like hard drives) to store and retrieve objects. Using **Docker Compose**, we configured MinIO to run locally, with a **bucket** (`raw-data`) where objects like `customers.csv` are stored. The bucket is physically stored on the local machine in the `./minio_data/` directory, simulating how objects would be stored in a real-world cloud environment. This local setup gives us a **single-node system**, where all data (objects and chunks) are stored on one machine, simulating a cloud storage environment without the complexity of a multi-node system. By using Docker, we are able to simulate the behavior of cloud storage systems, like AWS S3, in a contained, isolated environment, which makes it ideal for testing and development without needing cloud resources.

::: {.callout-important title="MinIO Bucket Features"}
-   **Buckets**: MinIO uses buckets to organize objects. A bucket is similar to a folder or directory in a filesystem, where each bucket can hold an arbitrary number of objects.

-   **Versioning**: Allows you to keep multiple versions of the same object under the same key.

-   **Object Locking**: Prevents objects from being deleted. Required to support retention and legal hold. Can only be enabled at bucket creation.

-   **Quota**: Limits the amount of data in the bucket.

-   **Retention**: Imposes rules to prevent object deletion for a defined period of time. Versioning must be enabled in order to set bucket retention policies.
:::

------------------------------------------------------------------------

# Pipeline structure

A typical modern data pipeline, especially in the context of e-commerce, involves several coordinated components working together to ensure consistent data ingestion, storage, processing, and orchestration.

------------------------------------------------------------------------

## Data Generation – *Frontend & Backend*

-   The website or mobile app (**frontend**) generates events such as clicks, views, and cart additions.
-   The **backend** handles order processing, user accounts, payments, and logistics.
-   These data points are tracked and collected in real-time or near-real-time.
-   Technologies: API endpoints, SDKs, webhooks, system logs, event collectors.

------------------------------------------------------------------------

## Ingestion – *From Events to Storage*

-   The data is ingested into a centralized system via:
    -   **Real-time ingestion** (e.g., Kafka, AWS Kinesis, GCP Pub/Sub)
    -   **Batch ingestion** (e.g., daily Python scripts, SQL jobs, ETL tools)
-   Sources may include:
    -   Relational databases
    -   Event streams
    -   File uploads

------------------------------------------------------------------------

## Storage – *Distributed Object Storage*

-   Raw data lands in an object storage system such as:
    -   Amazon S3
    -   Google Cloud Storage
    -   Azure Blob Storage
    -   MinIO (for local/testing environments)
-   Object storage ensures:
    -   **Scalability** (can handle large volumes)
    -   **Durability and availability** (reliable long-term storage)
    -   **Flexible formats** (e.g., Parquet, CSV, JSON)

------------------------------------------------------------------------

## Processing – *Spark-Based Analytics*

-   A distributed processing engine (typically **Apache Spark**) reads data from storage and performs:
    -   Data cleaning
    -   Transformations
    -   Aggregations
    -   Metric calculations
-   The output may be:
    -   Stored in analytical databases (e.g., BigQuery, Redshift, Snowflake)
    -   Used in BI dashboards
    -   Exported as new processed files

------------------------------------------------------------------------

## Orchestration – *Automation with Airflow*

-   All stages are scheduled and orchestrated with tools like:
    -   Apache Airflow
    -   Dagster
    -   Prefect
-   These tools handle:
    -   Task dependencies
    -   Execution schedules
    -   Error handling and retries
    -   Logging and monitoring

------------------------------------------------------------------------

## 🎯 Final Goals

-   **Business insights**: Understand user behavior and site performance.
-   **Machine learning**: Feed models for recommendations, forecasts, etc.
-   **Monitoring & auditing**: Track transactions and operations.
-   **Optimization**: Improve UX, conversion rates, and operations.